{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c016b52e",
   "metadata": {},
   "source": [
    "### Based on: Building makemore - Part 4 - Becoming a Backprop Ninja (https://github.com/karpathy/nn-zero-to-hero)\n",
    "> I implemented all the excerices from  https://www.youtube.com/watch?v=q8SA3rM6ckI. \n",
    "\n",
    "- ###### I fit randomly generated data\n",
    "- ###### I use a slightly different MLP architecture with one output neuron\n",
    "- ###### I use Binary Crossentropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12de324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55974088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 3]), torch.Size([1000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 1000\n",
    "num_features = 3\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xtrain = torch.randn(num_examples,num_features,generator=g) # random data\n",
    "y0 = torch.zeros(int(num_examples/2))\n",
    "y1 = torch.ones(int(num_examples/2))\n",
    "shuffle = torch.randperm(num_examples,generator=g) \n",
    "ytrain = torch.cat((y0,y1),dim=0)[shuffle] # random labels\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47e554b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3]), torch.Size([100]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation set\n",
    "val_examples = 100\n",
    "\n",
    "g = torch.Generator().manual_seed(0) # diaforetiko seed giana parw diaforetika data\n",
    "xval = torch.randn(val_examples,num_features,generator=g) # random test data\n",
    "\n",
    "val_y0 = torch.zeros(int(val_examples/2))\n",
    "val_y1 = torch.ones(int(val_examples/2))\n",
    "shuffle = torch.randperm(val_examples,generator=g) \n",
    "yval = torch.cat((val_y0,val_y1),dim=0)[shuffle] # random labels \n",
    "xval.shape, yval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1af82ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    }
   ],
   "source": [
    "# Neural Net parameters\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Layer 1\n",
    "W1 = torch.randn(num_features,n_hidden,generator=g) * (5/3) / (num_features**0.5)  # first layer weights\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # first layer biases # just for fun, einai perito logw tou BN\n",
    "# Layer 2\n",
    "W2 = torch.randn(n_hidden,1,generator=g) * 0.1 # output layer weights\n",
    "b2 = torch.randn(1,generator=g) * 0.1 # output layer biases\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1,n_hidden))*0.1 + 1 # Wraio Kolpo -> einai konta sto 1\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "# karpathy note:\n",
    "  # Note: I am initializating many of these parameters in non-standard ways\n",
    "  # because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "  # implementation of the backward pass\n",
    "\n",
    "# bnmean_running = torch.zeros((1,n_hidden))\n",
    "# bnstd_running = torch.ones((1,n_hidden))\n",
    "\n",
    "parameters = [W1, b1, W2, b2, bngain, bnbias] \n",
    "model_complexity = sum(p.nelement() for p in parameters) # i measure model complexity just as the number of parameters\n",
    "print(model_complexity)\n",
    "# so that i can run backbrop\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2070d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de24751",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # a shorter variable for convinience\n",
    "# construct a mini-batch\n",
    "ix = torch.randint(0,xtrain.shape[0],(batch_size,))\n",
    "xb, yb = xtrain[ix], ytrain[ix]\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d33894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_max = logits.view(-1).max()\n",
    "# norm_logits = logits - logits_max\n",
    "# logit = norm_logits[0]\n",
    "# sigmoid_den = (1 + (-norm_logits).exp())\n",
    "# prob = sigmoid_den ** -1\n",
    "# prob = prob.view(-1)\n",
    "# loss = - (yb * prob.log() + (1-yb) * (1-prob).log() ).mean()\n",
    "# print(loss)\n",
    "# loss.log10().item(), lossi[0]\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # Auta ta 2 einai to idio pragma - > mono gia ena stoixeio... gia dianisma prob den einai to idio!!!\n",
    "\n",
    "# # Prepei na iliopoiisw ti sigmoid prwta kai meta to ylog(y_hat) + (1-y)log(1-y_hat)\n",
    "# logprob = prob.log()\n",
    "# loss = - (yb * logprob).mean()\n",
    "# print(loss)\n",
    "# loss.log10().item(), lossi[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2145c53",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "- ##### TO LOSS PREPEI NA GINEI: -(pred.log() * y + (1-y) * (1-pred).log()).mean()\n",
    "- ##### ENAI LATHOS TO yb * logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c12a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6729, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "# Linear layer 1\n",
    "hprebn = xb @ W1 + b1\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n * hprebn.sum(dim=0,keepdims=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(dim=0,keepdims=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# binary cross entropy loss (same as F.binary_cross_entropy_with_logits(logits, Yb))\n",
    "sigmoid_den = 1 + (-logits).exp() # paronomastis tis sigmoid\n",
    "probs = sigmoid_den ** -1\n",
    "probs = probs.view(-1)\n",
    "term1 = probs.log() * yb\n",
    "term2 = (1-probs).log() * (1-yb)\n",
    "loss = -(term1 + term2).mean()\n",
    "\n",
    "# Pytorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "  #logprobs, norm_logits\n",
    "for t in [term1,term2, probs, sigmoid_den, # afaik there is no cleaner way\n",
    "        logits, h, hpreact, bnraw, bnvar_inv, bnvar, \n",
    "        bndiff2, bndiff, hprebn, bnmeani]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a8c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term1           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "term2           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "sigmoid_den     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# gia loss = -(yb*logprobs).mean() #-> to dnorm_logits edw vgainei 0.46 enw prepei na einai konta sto 0\n",
    "\n",
    "# dlogprobs = torch.zeros_like(logprobs) + yb * (-1.0/n)\n",
    "# dprobs = (1.0 / probs) * dlogprobs.view(dlogprobs.shape[0],1)\n",
    "# dsigmoid_den = -sigmoid_den**-2 * dprobs\n",
    "\n",
    "# Gia loss = -(probs.log()*y + (1-y)*(1-probs).log()).mean()\n",
    "\n",
    "dterm1 = torch.zeros_like(term1) - (1.0/n)\n",
    "dterm2 = torch.zeros_like(term2) - (1.0/n)\n",
    "dprobs = (1.0/probs) * yb * dterm1\n",
    "dprobs += -(1.0/(1-probs)) * (1-yb) * dterm2\n",
    "dsigmoid_den = (-sigmoid_den**-2) * dprobs.view(-1,1) # undo to view(-1), to kanw (32,1) apo (32)\n",
    "dlogits = -((-logits).exp()) * dsigmoid_den\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(dim=0,keepdims=True)\n",
    "dbnbias = dhpreact.sum(dim=0,keepdims=True) \n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (bndiff*dbnraw).sum(dim=0,keepdims=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar = -(0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "dbnmeani = (-1.0 * dbndiff).sum(dim=0,keepdims=True) # or (-torch.ones_like(bndiff) * dbndiff).sum(dim=0) = (-dbndiff).sum(dim=0)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += (1.0/n) * torch.ones_like(hprebn) * dbnmeani\n",
    "dW1 = xb.T @ dhprebn\n",
    "db1 = dhprebn.sum(dim=0)\n",
    "\n",
    "cmp('term1', dterm1, term1)\n",
    "cmp('term2', dterm2, term2)\n",
    "# cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('sigmoid_den', dsigmoid_den, sigmoid_den)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffd66b",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### The following cells contain Notes and intermiediate results used for calculating many of the above expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "247b7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dloprobs\n",
    "# loss = - 1/3 * (a + b + c) * yb = -1/3a*yb - 1/3b*yb - 1/3c*yb\n",
    "# dloss/da  = -1/3 * yb\n",
    "# sti geniki periptosi dloss/da = yb * 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dprobs\n",
    "# y = log(x)\n",
    "# dy/dx = 1/x - > local derivative 1.0/ probs, to view einai gia na kanw undo to view(-1)!\n",
    "# 1.0/ probs * dlogprobs.view(dlogprobs.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsigmoid_den\n",
    "# probs = 1/ sigmoid_den\n",
    "# dprobs/dsigmoid_den = -(1/sigmoid_den)**2 -> local derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnorm_logits\n",
    "# y = 1 + e**(-x)\n",
    "# dy/dx = -e**(-x) -> local derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ca0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogit_maxes\n",
    "# dnorm_logit/dlogit_maxes = -1.0 - > local derivative\n",
    "# logits.shape = (32,1) , logits_maxes.shape = (1,1)\n",
    "# kanonika tha itan -1.0 * dnorm_logits, alla epeidei dnorm_logits.shape = (32,1) \n",
    "# enw egw thelw dlogit_maxes.shape == logit_maxes.shape = (1,1)\n",
    "# to logit_maxes xrisimopoieitai polles fores gia oles tis grammes tou logits -> broadcasting\n",
    "# otan kati (enas komvos sto graph) xrisimopoieitai polles fores \n",
    "# sto backprop ta grad tha athroizontai (sum) gia kathe branch\n",
    "# gi auto pairnw sum kata grammes me keepdim wste na parw (1,1)\n",
    "# dnorm_logits = (-1.0 * dnorm_logits.sum(0,keepdim=True))\n",
    "\n",
    "# dlogit_maxes = 0.3611 EPREPE NA EINAI POLI KONTA STO 0.... kati kanw lathos?? \n",
    "# mipws den xreiazetai katholou to step me to normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlogits\n",
    "\n",
    "# dlogits = 1 - > local derivative\n",
    "# alla epeidi logit_maxes = f(logits) exw kai deutero branch sto logits \n",
    "# kai auti den einai i teliki derivative gia to logits\n",
    "\n",
    "# c1 = a1 - b\n",
    "# c2 = a2 - b\n",
    "# c3 = a3 - b\n",
    "\n",
    "# to b einai to max kai ginetai broadcast kata grammmes gi auto kanw sum(dim=0) sto dlogit_maxes\n",
    "# c1 = a1 - b\n",
    "\n",
    "# gia to max to local derivative einai 1 gia to mono gia to stoixeio max\n",
    "# xrisimopoiw to logits.max(0).indices gia to index tou max kai ftiaxnw ena dianisma stili wste na pol/stei me to dlogit_maxes\n",
    "# F.one_hot.shape = (32,1) , dlogit_maxes.shape = (1,1) kai ginetai broadcasting swsta! -> to logit_maxes antigrafetai kata grammes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bndiff2 \n",
    "# a11 a12\n",
    "# a21 a22\n",
    "# ---->\n",
    "# b1 b2 , where\n",
    "# b1 = 1/(n-1) * (a11 + a21)\n",
    "# b2 = 1/(n-1) * (a12 + a22)\n",
    "# to local derivative einai 1/(n-1) gia kathe a\n",
    "# the derivative of b1 has to flow through the columns of a scaled me to 1/(n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dhprebn\n",
    "# opws kai to dbndiff2 prepei na kanw flow to gradient kata grammes epi to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ac56d",
   "metadata": {},
   "source": [
    "### End of cells containing Notes\n",
    "### ---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700af0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6728731989860535 diff: -5.960464477539063e-08\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# sigmoid_den = 1 + (-logits).exp()\n",
    "# probs = sigmoid_den ** -1\n",
    "# probs = probs.view(-1)\n",
    "# term1 = probs.log() * yb\n",
    "# term2 = (1-probs).log()* (1-yb)\n",
    "# loss = -(term1 + term2).mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.binary_cross_entropy_with_logits(logits.view(-1), yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f68ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "dlogits = F.sigmoid(logits).view(-1) # ta probs....\n",
    "dlogits = (dlogits - yb).view(-1,1)\n",
    "dlogits /= n\n",
    "cmp('logits', dlogits, logits) \n",
    "\n",
    "# d/dx(y*log(x) + (1-y)log(1-x)) = - (x - y) / (1-x)*x\n",
    "# dloss/dlogits = dloss/probs * dprobs/dlogits\n",
    "# dloss/dprobs = dprobs = (probs - yb) / (probs * (1-probs)), apo tin apo panw sxesi (WOlfram alpha)\n",
    "# dprobs/dlogits = sigmoid' = s*(1-s) = probs * (1-probs)\n",
    "# An ta pol/sw auta aplopoiointai ola kai menei mono (probs-yb) to scalarw /n epeidi eixa to .mean()\n",
    "# to view einai gia na ksanapaw se shape [32, 1] apo [32] (epd eixa probs.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d81a78e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4334, -0.4425,  0.2401, -0.5814, -0.0443, -0.2771,  0.0244, -0.2647,\n",
       "          0.1349,  0.1487,  0.0642, -0.0462,  0.3320,  0.5255,  0.5218,  0.1915,\n",
       "         -0.6503,  0.2971, -0.0056,  0.2296, -0.0087, -0.5033, -0.0928,  0.1861,\n",
       "         -0.5740, -0.7704, -0.8233, -0.9068, -0.4128,  0.4073, -0.8394,  0.0081],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([-0.6067, -0.6089, -0.4403,  0.3586,  0.4889, -0.5688,  0.5061, -0.5658,\n",
       "          0.5337,  0.5371, -0.4840,  0.4884,  0.5823,  0.6284, -0.3724, -0.4523,\n",
       "          0.3429,  0.5737, -0.5014,  0.5572,  0.4978,  0.3768,  0.4768,  0.5464,\n",
       "          0.3603,  0.3164, -0.6949, -0.7124,  0.3982,  0.6004, -0.6983,  0.5020],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(-1), dlogits.view(-1)*n, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57314e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa09ac57820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e02f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(dim=0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(dim=0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a05e8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbnvar_inv = (bndiff*dbnraw).sum(dim=0,keepdims=True)\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar = -(0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += 2 * bndiff * dbndiff2\n",
    "# dbnmeani = (-1.0 * dbndiff).sum(dim=0,keepdims=True) # or (-torch.ones_like(bndiff) * dbndiff).sum(dim=0) = (-dbndiff).sum(dim=0)\n",
    "# dhprebn = dbndiff.clone()\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(dim=0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(dim=0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3045890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100]),\n",
       " torch.Size([1, 100]),\n",
       " torch.Size([1, 100]),\n",
       " torch.Size([32, 100]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d9345dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 0.6906\n",
      "  10000/ 200000: 0.6768\n",
      "  20000/ 200000: 0.6921\n",
      "  30000/ 200000: 0.6807\n",
      "  40000/ 200000: 0.7058\n",
      "  50000/ 200000: 0.5915\n",
      "  60000/ 200000: 0.7081\n",
      "  70000/ 200000: 0.6851\n",
      "  80000/ 200000: 0.6553\n",
      "  90000/ 200000: 0.6856\n",
      " 100000/ 200000: 0.6559\n",
      " 110000/ 200000: 0.5940\n",
      " 120000/ 200000: 0.7083\n",
      " 130000/ 200000: 0.6186\n",
      " 140000/ 200000: 0.6467\n",
      " 150000/ 200000: 0.6576\n",
      " 160000/ 200000: 0.6764\n",
      " 170000/ 200000: 0.6135\n",
      " 180000/ 200000: 0.7151\n",
      " 190000/ 200000: 0.6758\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# Neural Net parameters\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# Layer 1\n",
    "W1 = torch.randn(num_features,n_hidden,generator=g) * (5/3) / (num_features**0.5)  # first layer weights\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # first layer biases # just for fun, einai perito logw tou BN\n",
    "# Layer 2\n",
    "W2 = torch.randn(n_hidden,1,generator=g) * 0.1 # output layer weights\n",
    "b2 = torch.randn(1,generator=g) * 0.1 # output layer biases\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1,n_hidden))*0.1 + 1 # Wraio Kolpo -> einai konta sto 1\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "parameters = [W1, b1, W2, b2, bngain, bnbias] \n",
    "model_complexity = sum(p.nelement() for p in parameters) # i measure model complexity just as the number of parameters\n",
    "# so that i can run backbrop\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "epochs = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # a shorter variable for convinience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  for i in range(epochs):\n",
    "    ix = torch.randint(0,xtrain.shape[0],(batch_size,))\n",
    "    xb, yb = xtrain[ix], ytrain[ix]\n",
    "\n",
    "    # forward pass\n",
    "    # Linear layer\n",
    "    hprebn = (xb @ W1) + b1\n",
    "    # BatchNorm layer\n",
    "    # ---------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # ---------------------------------------------------------------\n",
    "    # Non-Linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "  #   h.retain_grad() # gia na borw na kanw h.grad\n",
    "    logits = (h @ W2) + b2\n",
    "    loss = F.binary_cross_entropy_with_logits(logits.view(-1),yb) # mallon pio numericaly stable?\n",
    "\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.sigmoid(logits).view(-1)\n",
    "    dlogits = (dlogits - yb).view(-1,1)\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(dim=0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(dim=0,keepdims=True)\n",
    "    dbnbias = dhpreact.sum(dim=0,keepdims=True)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(dim=0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(dim=0))\n",
    "    # 1st layer\n",
    "    dW1 = xb.T @ dhprebn\n",
    "    db1 = dhprebn.sum(dim=0)\n",
    "\n",
    "    grads = [dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.09 # decay the lr?\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    #track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i>=100:\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7610a0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(100,)          | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-09\n",
      "(100, 1)        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "(1,)            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "(1, 100)        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "(1, 100)        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636a1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  hpreact = xtrain @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1539fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.6148810982704163\n",
      "val 0.7544530630111694\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (xtrain, ytrain),\n",
    "    'val': (xval,yval)\n",
    "  }[split]\n",
    "  hpreact = x @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, 1)\n",
    "  loss = F.binary_cross_entropy_with_logits(logits.view(-1), y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b887e99",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "- ###### Me loss.backward:\n",
    "    - train: 0.6025290489196777\n",
    "    - val: 0.7441983819007874\n",
    "- ###### Manual backprop:\n",
    "    - train: 0.6148810982704163\n",
    "    - val: 0.7544530630111694\n",
    "    \n",
    "###### NOT BAD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f088c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# me loss.backward:\n",
    "# train 0.6025290489196777\n",
    "# val 0.7441983819007874\n",
    "\n",
    "# manual backprop:\n",
    "# train 0.6148810982704163\n",
    "# val 0.7544530630111694\n",
    "\n",
    "# NOT BAD!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cf214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
